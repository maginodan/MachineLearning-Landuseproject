{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOv5011ZD/1h5xJdajCPQeg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["train_dir = '/content/drive/MyDrive/data_vit/split_dataset/train'\n","test_dir = '/content/drive/MyDrive/data_vit/split_dataset/test'"],"metadata":{"id":"VBFyzrxbFC4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"CkrTi4t5CAae","executionInfo":{"status":"error","timestamp":1714649607995,"user_tz":-180,"elapsed":366,"user":{"displayName":"owinymarvin tmt","userId":"16306066818364838244"}},"outputId":"094f23f1-11ff-4431-fd85-427d935755ec"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/data_vit/split_dataset/train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-dbece1b7a9c9>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m# Define the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-dbece1b7a9c9>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(train_dir, test_dir, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Create datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data_vit/split_dataset/train'"]}],"source":["# !pip install torchinfo\n","# !pip install captum\n","# !pip install lime\n","# !pip install shap\n","import os\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","from captum.attr import LayerGradCam, IntegratedGradients, visualization as viz\n","from lime import lime_image\n","import shap\n","from PIL import Image\n","import numpy as np\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Define your data preprocessing function\n","def preprocess_data(train_dir, test_dir, batch_size, num_workers=4):\n","    # Define transforms for data preprocessing\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    # Create datasets\n","    train_data = datasets.ImageFolder(train_dir, transform=transform)\n","    test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","    # Split the training data into training and validation sets\n","    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    return train_loader, val_loader, test_loader\n","\n","# Define your ViT model\n","class ViT(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ViT, self).__init__()\n","        self.model = torchvision.models.vision_transformer.vit_base_patch16_224_in21k(pretrained=True)\n","        self.model.head = nn.Linear(self.model.head.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Define the function to train the model\n","def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs, device):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = loss_fn(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * images.size(0)\n","        train_loss /= len(train_loader.dataset)\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = loss_fn(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","            val_loss /= len(val_loader.dataset)\n","            val_losses.append(val_loss)\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","    return train_losses, val_losses\n","\n","# Define the function to evaluate the model\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    print(f\"Test Accuracy: {accuracy:.2f}%\")\n","\n","# Define the function to generate explanations using Grad-CAM\n","def generate_grad_cam_explanations(model, image, target_class, device):\n","    model.eval()\n","    layer_grad_cam = LayerGradCam(model.model, model.model.encoder.blocks[-1].norm1)\n","    attributions_grad_cam = layer_grad_cam.attribute(image, target=target_class)\n","    vis = viz.visualize_image_attr_multiple(np.transpose(attributions_grad_cam.squeeze().cpu().detach().numpy(), (1,2,0)),\n","                                             np.transpose(image.squeeze().cpu().detach().numpy(), (1,2,0)),\n","                                             [\"original_image\", \"heat_map\"],\n","                                             [\"all\", \"positive\"],\n","                                             cmap=\"viridis\",\n","                                             show_colorbar=True)\n","    plt.imshow(vis)\n","    plt.show()\n","\n","# Define the function to generate explanations using Integrated Gradients\n","def generate_integrated_gradients_explanations(model, image, target_class, device):\n","    model.eval()\n","    integrated_gradients = IntegratedGradients(model.model)\n","    attributions_ig, delta = integrated_gradients.attribute(image, target=target_class, return_convergence_delta=True)\n","    vis_ig = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n","                                               np.transpose(image.squeeze().cpu().detach().numpy(), (1,2,0)),\n","                                               [\"original_image\", \"heat_map\"],\n","                                               [\"all\", \"positive\"],\n","                                               cmap=\"viridis\",\n","                                               show_colorbar=True)\n","    plt.imshow(vis_ig)\n","    plt.show()\n","\n","# Define the function to generate explanations using LIME\n","def generate_lime_explanations(model, image_path):\n","    model.eval()\n","    explainer = lime_image.LimeImageExplainer()\n","    image = Image.open(image_path)\n","    image = transform(image).unsqueeze(0).to(device)\n","    lime_explanation = explainer.explain_instance(image[0].cpu().permute(1, 2, 0).numpy(),\n","                                                   model,\n","                                                   top_labels=5,\n","                                                   hide_color=0,\n","                                                   num_samples=1000)\n","    explanation, mask = lime_explanation.get_image_and_mask(lime_explanation.top_labels[0],\n","                                                             positive_only=True,\n","                                                             num_features=5,\n","                                                             hide_rest=False)\n","    plt.imshow(explanation)\n","    plt.show()\n","\n","# Define the function to generate explanations using SHAP\n","def generate_shap_explanations(model, image_path):\n","    model.eval()\n","    shap_explainer = shap.Explainer(model, image, output_rank_order='max')\n","    shap_values = shap_explainer.shap_values(image)\n","    shap.image_plot(shap_values, -image.numpy(), labels=class_names)\n","\n","# Define paths to your train and test directories\n","\n","train_dir ='/content/drive/MyDrive/data_vit/split_dataset/train'\n","test_dir = '/content/drive/MyDrive/data_vit/split_dataset/test'\n","\n","# Define hyperparameters\n","batch_size = 32\n","num_epochs = 10\n","learning_rate = 1e-3\n","\n","# Preprocess the data\n","train_loader, val_loader, test_loader = preprocess_data(train_dir, test_dir, batch_size)\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create the ViT model\n","num_classes = len(train_loader.dataset.classes)\n","model = ViT(num_classes).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device)\n","\n","# Evaluate the model\n","evaluate_model(model, test_loader, device)\n","\n","# Generate explanations using Grad-CAM\n","image_path = 'example_image.jpg'  # Provide the path to your example image\n","image = Image.open(image_path)\n","image = transform(image).unsqueeze(0).to(device)\n","generate_grad_cam_explanations(model, image, target_class=0, device=device)\n","\n","# Generate explanations using Integrated Gradients\n","generate_integrated_gradients_explanations(model, image, target_class=0, device=device)\n","\n","# Generate explanations using LIME\n","generate_lime_explanations(model, image_path)\n","\n","# Generate explanations using SHAP\n","generate_shap_explanations(model, image_path)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1Kls1rycEF8U"},"execution_count":null,"outputs":[]}]}